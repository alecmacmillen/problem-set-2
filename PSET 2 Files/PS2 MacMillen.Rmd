---
title: "MACS 40500 - PS2"
author: "Alec MacMillen"
date: "11/11/2019"
output: pdf_document
---

```{r setup, include=FALSE}
rm(list = ls())
knitr::opts_chunk$set(echo = TRUE)

library(ggfortify)
library(caret)
library(glmnet)
library(haven)
library(foreign) 
library(MASS)
library(class)
library(faraway)
library(ggplot2)
library(arm)
library(OOmisc)
library(pROC)
library(wnominate)
library(pscl)
library(mlbench)
library(ggfortify)
library(ggpubr)
library(coefplot)
library(grid)
library(gridExtra)
library(tidyverse)
```

### Preparation: Evaluation functions

```{r}
precision.conf <- function(predicted, observed) {
  t <- as_tibble(table(observed, predicted))
  fp <- t$n[3]; tp <- t$n[4]
  return(tp / (tp + fp))
}

recall.conf <- function(predicted, observed) {
  t <- as_tibble(table(observed, predicted))
  tn <- t$n[1]; tp <- t$n[4]
  return(tp / (tp + tn))
}

fscore.conf <- function(predicted, observed, beta=1) {
  t <- as_tibble(table(observed, predicted))
  tn <- t$n[1]; fn <- t$n[2]; fp <- t$n[3]; tp <- t$n[4]
  f <- ((1 + beta^2) * tp) / ((1 + beta^2) * tp + beta^2 * fn + fp)
  return(f)
}
```

I created a few model evaluation functions for measuring precision, recall, and the F-score. There are existing functions, but these custom versions are useful when paired with the quick-and-dirty confusion matrices constructed by tabling actual and predicted classifications of 0's and 1's.


### Part 1: Classification

```{r}
data <- haven::read_dta("conf06.dta")

conf <- data %>% 
  filter(nominee != "ALITO") %>% 
  dplyr::select(vote, nominee, sameprty, qual, lackqual, EuclDist2, strngprs) %>% 
  mutate(numvote = as.numeric(vote),
         numstrngprs = as.numeric(strngprs),
         numsameprty = as.numeric(sameprty))
```

Alito confirmation was filtered out to restrict analysis to confirmations up to John Roberts. Although the problem set suggested subtracting 1 from the $strngprs$ and $sameprty$ variables to encode them on a 0-1 scale, they appeared to already be so when I read the data in so that step was not necessary.

#### 1. Define 80/20 train/test split
```{r}
samples <- sample(1:nrow(conf), nrow(conf)*0.8, replace = FALSE)
train.data <- conf[samples, ]
test.data <- conf[-samples, ]
vote <- test.data$numvote
```

An 80%-20% train-test split is achieved using a random sampling technique, which randomly assigns 80% of observations to the training set and 20% of observations to the testing set. Although this means there might be some observations in the training set that occurred *after* observations in the testing set (i.e., we're using future events to "predict" past events), this is not a problem so long as we can feasibly assume that the underlying distribution from which senators' confirmation votes are drawn is fundamentally static over time. That is, time should not alter the relationship between our predictive features and vote outcomes.

#### 2. Build logit classifier
```{r}
logit <- glm(numvote ~ numsameprty + lackqual + EuclDist2 + 
               numstrngprs + lackqual*EuclDist2, 
             data = train.data, 
             family = binomial)

logit.probs <- predict(logit, newdata = test.data, type="response") 
logit.pred <- ifelse(logit.probs > 0.5, 1, 0)

# Confusion matrix
table(logit.pred, vote)
# Accuracy rate
mean(logit.pred == vote)
# Precision
precision.conf(logit.pred, vote)
# Recall
recall.conf(logit.pred, vote)
# Fscore
fscore.conf(logit.pred, vote)
```

The logistic regression classifier uses indicators to signal whether the president that named the nominee was from the same party as the Senator in question and for whether the president was "strong" (i.e. his party controlled the Senate and he was not in his fourth year of office), in addition to a measure of the ideological distance between the Senator and nominee, a measure of perceived lack of the nominee's qualifications, and an interaction between ideological distance and perceived qualification. 

The logistic regression classifier appears to perform pretty well: it has a raw accuracy rate of 0.911, precision of 0.918, recall of 0.950, and an F1-score of 0.951. The confusion matrix indicates that in the test set the vast majority of observations fall into true positives, with very few false negatives. There are more false positives than true negatives, which suggests that senators sometimes vote against nominees that our classifier would expect them to vote in favor of - perhaps for the purpose of ideological grandstanding.

#### 3. Build LDA classifier
```{r}
lda <- lda(numvote ~ numsameprty + lackqual + EuclDist2 + 
               numstrngprs + lackqual*EuclDist2, 
           data = train.data)

lda
plot(lda, filename = "lda.png")

lda.pred <- predict(lda, newdata=test.data) 

# confusion matrix
table(lda.pred$class, vote)
# check the classification rate
mean(lda.pred$class == vote)
# Precision
precision.conf(lda.pred$clas, vote)
# Recall
recall.conf(lda.pred$clas, vote)
# Fscore
fscore.conf(lda.pred$clas, vote)
```
The linear discriminant analysis classifier performs similarly to logistic regression, with accuracy, precision, recall, and F1 metrics above 0.90. The prior class probabilities suggest that senators vote in favor of nominees most of the time, which holds with our historical understandings that SCOTUS confirmations pre-Bork were largely collegial, unified affairs. The hyperpartisan and dogmatic nature of recent confirmation hearings are not enough to cancel out this earlier trend. Additionally, the large magnitude of the coefficient on the interaction term between the $lackqual$ and $EuclDist2$ features suggest that senators might make different calculations about whether or not to vote in favor of an "unqualified" nominee if that nominee is ideologically closer to the senator.

#### 4. Calculate and plot predicted probabilities

```{r}
# Find range of values of qual variable
summary(conf$qual)

# Create synthetic dataset with range of qual values, holding all others constant
# at their average values
conf2 <- with(conf, data.frame(
  lackqual = seq(from = .11, to = 1, length.out = 100),
  numsameprty = mean(numsameprty),
  numstrngprs = mean(numstrngprs),
  EuclDist2 = mean(EuclDist2))) 

# Use logit model to predict confirmation vote likelihood
conf3 <- cbind(conf2, predict(logit, newdata = conf2, type = "link", se = TRUE))

# Add CIs and predicted points for error bar plotting
conf3 <- within(conf3, {
  PredictedProb <- plogis(fit)
  LL <- plogis(fit - (1.96 * se.fit))
  UL <- plogis(fit + (1.96 * se.fit))
})

# Plot confirmation vote probability as a function of nominee qualification
# with error bars included
ggplot(conf3, aes(x = lackqual, y = PredictedProb)) +
  geom_line() +
  geom_errorbar(aes(x = lackqual, ymin = LL, ymax = UL)) +
  labs(x = "Perceived Lack of Nominee Qualifications",
       y = "Probability of Vote to Confirm") +
  ggtitle("The Conditional Effect of Nominee Qualification on Vote to Confirm") +
  theme_bw()
```
According to our logistic regression classifier, as nominees' qualifications lessen, the probability that a given senator will vote to confirm them decreases. For highly qualified candidates (with the "lack of qualification" measure less than 0.25), the odds that a given senator will vote in favor of confirmation are above 90%, with very little imprecision in estimation. However, as the perceived lack of qualification moves toward 1, the odds of a confirmation vote fall at an increasing rate until they are essentially a coin flip (roughly 50% with a confidence interval of 40%-60%). When paired with the knowledge that ideological distance is an important determinant of likelihood of a "yes" vote, this pattern suggests that the Senate is fairly unified even across partisan lines in the face of an undeniably qualified nominees, but that votes are significantly more uncertain and could cleave along partisan lines when nominees are perceived as less qualified..

#### 5. Discussion of Results

Perhaps unsurprisingly, ideological distance and perceived qualification are the most important determinants of whether a given senator will vote in favor of a given SCOTUS nominee. The confusion matrices of our two classifiers suggest that type 1 errors (false positives) are more common than type 2 errors (false negatives), meaning that our classifiers are more likely to erroneously predict that a senator will vote in favor of a nominee that they actually choose to oppose, rather than make the opposite type of error. This makes intuitive sense, because far and away the largest category of predicted vs. actual outcomes are the true positives - most senators in the dataset vote to confirm most nominees, so our classifiers will predict positive outcomes more often and thus make type 1 errors more often.

The plot in part (4) also depicts a nonlinear relationship between perceived lack of nominee qualification and likelihood of confirmation vote, with less-qualified nominees less likely to receive "yea" votes. Interestingly, even the least-qualified candidates are predicted to have a roughly 40%-60% chance of receiving a "yea" vote for a given senator, suggesting that senators will vote for even unqualified nominees under certain circumstances - for example, if that nominee is ideologically "close" to the senator and is likely to vote in a way that senator approves of.

#### 6. Bonus question

```{r}
# Create synthetic dataset with range of qual values and numsameprty, 
# to account for ixn term, holding all others constant at their average values
conf2.2 <- with(conf, data.frame(
  lackqual = rep(seq(from = .11, to = 1, length.out = 100), 2),
  numsameprty = rep(0:1, each = 100),
  numstrngprs = mean(numstrngprs),
  EuclDist2 = mean(EuclDist2)))

# Predicted values using synthetic dataset
conf3.2 <- cbind(conf2.2, predict(logit, newdata = conf2.2, type = "link", se = TRUE))

# Add CIs and predicted points for error bar plotting
conf3.2 <- within(conf3.2, {
  PredictedProb <- plogis(fit)
  LL <- plogis(fit - (1.96 * se.fit))
  UL <- plogis(fit + (1.96 * se.fit))
})

# Recode numsameprty as a factor
conf3.2$numsameprty <- factor(conf3.2$numsameprty, labels=c("No", "Yes"))

# Plot predictions with CIs with separate curves by same-party status
ggplot(conf3.2, aes(x = lackqual, y = PredictedProb, color = numsameprty)) +
  geom_line() +
  geom_errorbar(aes(x = lackqual, ymin = LL, ymax = UL)) +
  labs(x = "Perceived Lack of Nominee Qualifications",
       y = "Probability of Vote to Confirm",
       color = "Senator is same party as president") +
  scale_fill_hue(breaks = c("No", "Yes"),
                 labels = c("No",  "Yes")) +
  ggtitle("The Conditional Effect of Nominee Qualification on Vote to Confirm") +
  theme_bw() +
  theme(legend.justification = c(.7,1),
        legend.position = c(.35,.3))
```
This plot splits out the qualification-vote likelihood relationship by whether or not the nominee is in the same party as the voting senator, and presents an unsurprising result: senators are more likely to overlook a lack of qualification and vote in favor of a SCOTUS nominee if that senator comes from the same party as the president that named the nominee. Ideological considerations apparently still exert strong influence over confirmation votes, because senators are more likely to vote "nay" if they are in the opposing party to the president at any level of perceived nominee qualification. 

### Part 2: Unfolding

#### Read in data

```{r}
house113 <- readKH("hou113kh.ord", dtl = NULL, yea = c(1,2,3),
                   nay = c(4,5,6), missing = c(7,8,9), notInLegis=0,
                   desc="113th_House_Roll_Call_Data", debug=FALSE)
```

#### 1. Fit and analyze W-NOMINATE algorithm

```{r}
wnom_result <- wnominate(house113, dims = 2, minvotes = 20, lop = 0.025, polarity = c(2,2)) 

wnom1 <- wnom_result$legislators$coord1D 
wnom2 <- wnom_result$legislators$coord2D 
party <- house113$legis.data$party 

plot(wnom1, wnom2,
     main="113th United States House\n(W-NOMINATE)",
     xlab="First Dimension (Ideology) \nD = Democrat, R = Republican, I = Independent",
     ylab="Second Dimension (Race / Civil Rights)",
     xlim=c(-1,1), ylim=c(-1,1), type="n")
points(wnom1[party=="D"], wnom2[party=="D"], pch="D", col="gray15")
points(wnom1[party=="R"], wnom2[party=="R"], pch="R", col="gray30")
points(wnom1[party=="Indep"], wnom2[party=="Indep"], pch="I", col="red")
```

At the beginning of the 113th Congress, Republicans held a 233-200 majority. Despite the fact that their conference was larger than the Democratic caucus, Republicans appeared to be more ideologically homogenous, with their members more tightly clustered on the ideology scale, between 0.5 (center right) and 1 (far right). Democrats were much more spread out between 0 (center) and -1 (far left), perhaps signaling the presence of more diverse "wings" of the party.

#### 2. Space Dimensionality

```{r}
summary(wnom_result)
plot.nomObject(wnom_result)
par(mfrow = c(1,1))
```
The scree plot indicates that, although the 113th House can be modeled in ${\rm I\!R}^2$, ideology is far and away the single most important dimension in classifying members of Congress based on their roll-call votes. The eigenvalue for the second-most important dimension is barely 1 and is virtually indistinguishable for the third-most important dimension (and the same is largely true for subsequent eigenvalues). The values of the aggregate proportion reduction in errors barely changes from one dimension to two (0.817 to 0.837) and the same is true for the geometric mean prediction (0.840 in one dimension and 0.857 in two). This suggests that, while adding a second dimension increases the fit and predictive power of the W-NOMINATE algorithm, these gains in performance are minimal and any choice of a second dimension is dwarfed in terms of importance by base ideology.

#### 3. Advantages and disadvantages of unfolding approaches

Politics and ideological expression generally occurs in latent space, which is difficult to observe. Placement on an ideological scale is not absolute: it is relative to time, context, and other actors. An unfolding approach allows us to inductively rank or place members in an observable space and is especially useful when we're trying to understand ideological clusters within some body that regularly engages in voting behavior that we can use to constantly update our understanding of placements. In general, we require that legislators vote their beliefs sincerely rather than acting strategically, and that their preferences for many diverse issues can be reduced down to a lower-dimensional basic space that we can both understand and glean some type of meaning from.

An unfolding approach is less appropriate when we have a manifestly apparent outcome (i.e. non-latent), or when we cannot reasonably assume that a high-dimensional space we're modeling can be reduced to a lower-dimensional basic space while still retaining useful properties that allow us to draw reliable inferences about our population of interest. Similarly, if we assume that actors (legislators) express preferences (vote decisions) strategically rather than sincerely - or God forbid, have an intransitive preferences - it is difficult to correctly rank their preferences and it becomes difficult if not impossible to take a maximum likelihood approach to estimating their ideal points.










